{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B6Gk1tOY6Sd9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from random import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAhlQx-i6SeC"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int,\n",
    "                 num_layers: int=1, batch_first: bool=True, dropout: float=0):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size,\n",
    "                            num_layers=num_layers, batch_first=batch_first, dropout=dropout)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, h0, c0):\n",
    "        hidden_states, (h, c) = self.lstm(x, (h0, c0))\n",
    "        return hidden_states, h, c\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int,\n",
    "                 num_layers: int=1, batch_first: bool=True, lstm_dropout: float=0, dropout_layer_dropout: float=0):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size,\n",
    "                            num_layers=num_layers, batch_first=batch_first, dropout=lstm_dropout)\n",
    "        self.dropout_layer = nn.Dropout(dropout_layer_dropout)\n",
    "        self.layernorm = nn.LayerNorm(hidden_size)\n",
    "        self.linear=nn.Linear(hidden_size, output_size)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x, h0, c0):\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(1) if self.batch_first else x.unsqueeze(0)\n",
    "\n",
    "        outputs, (h, c) = self.lstm(x, (h0, c0)) # outputs.shape = (batch_size, 1, hidden_size) if batch_first = True\n",
    "                                                 #               = (1, batch_size, hidden_size) if batch_first = False\n",
    "        outputs = self.dropout_layer(outputs)\n",
    "        outputs = self.layernorm(outputs)\n",
    "        y = self.linear(outputs)\n",
    "        return y, h, c\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, decoder_output_size, hidden_size, W0=None):\n",
    "        super().__init__()\n",
    "        if W0 is not None and W0.shape == (decoder_output_size, hidden_size):\n",
    "            self.W = nn.Parameter(W0)\n",
    "        else:\n",
    "            self.W = nn.Parameter(torch.empty(decoder_output_size, hidden_size))\n",
    "            nn.init.xavier_uniform_(self.W)\n",
    "\n",
    "    def forward(self, h, encoder_hidden_states, batch_first):\n",
    "        # h is the current hidden state of the decoder; h.shape = (decoder_num_layers, batch_size, decoder_output_size)\n",
    "        # encoder_hidden_states.shape = (batch_size, L, hidden_size) if batch_first else (L, batch_size, hidden_size), L is source (input) sequence length\n",
    "        # self.W should be of shape (decoder_output_size, hidden_size)\n",
    "        h_last = h[-1]\n",
    "\n",
    "        if not batch_first:\n",
    "            encoder_hidden_states = encoder_hidden_states.transpose(0, 1)\n",
    "\n",
    "        scores = torch.einsum(\"bd, dh, blh -> bl\", h_last, self.W, encoder_hidden_states)  # scores.shape = (batch_size, L)\n",
    "        attn_weights = F.softmax(scores, dim=1)  # attn_weights.shape = (batch_size, L)\n",
    "        context_vec = torch.einsum(\"bl, blh -> bh\", attn_weights, encoder_hidden_states) # context_vec.shape = (batch_size, hidden_size)\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, attention):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.encoder = encoder.to(self.device)\n",
    "        self.decoder = decoder.to(self.device)\n",
    "        self.attention = attention.to(self.device)\n",
    "\n",
    "    # source = input to the encoder; x0 = initial input to initialize the decoder\n",
    "    # Y = ground truths; Y.shape = (batch_size, L, decoder.output_size) if batch_first=True\n",
    "    #                            = (L, batch_size, decoder.output_size) if batch_first = False\n",
    "    def forward(self, source, x0, h0, c0,\n",
    "                transform: callable=lambda x: x, teacher_forcing_ratio: float=0.0, Y=None, L=None):  # L is target sequence length\n",
    "        source, x0, h0, c0 = source.to(self.device), x0.to(self.device), h0.to(self.device), c0.to(self.device)\n",
    "        batch_first = self.encoder.batch_first\n",
    "        if Y is None:\n",
    "            if L is None:\n",
    "                print(\"Target sequence length not provided. Defaulting to input sequence length.\")\n",
    "                L = source.shape[1] if batch_first else source.shape[0]\n",
    "            if teacher_forcing_ratio != 0.0:\n",
    "                print(\"For teacher_forcing_ratio=0, Y must be None\")\n",
    "                return None\n",
    "        else:\n",
    "            L = Y.shape[1] if batch_first else Y.shape[0]\n",
    "            Y = Y.to(self.device)\n",
    "\n",
    "        batch_size = source.shape[0] if batch_first else source.shape[1]\n",
    "        decoder_output_size = self.decoder.output_size\n",
    "\n",
    "        if Y is not None and batch_first:\n",
    "            Y = Y.transpose(0,1) # for indexing convenience\n",
    "\n",
    "        encoder_hidden_states, h, c = self.encoder(source, (h0, c0))  # h and c of shape (num_layers, batch_size, hidden_size); hidden_size = encoder_hidden_size = decoder_hidden_size\n",
    "        context_vec = self.attention(h, encoder_hidden_states, batch_first)\n",
    "        context_vec = context_vec.unsqueeze(1) if batch_first else context_vec.unsqueeze(0) # context_vec.shape becomes (batch_size, 1, hidden_size) if batch_first=True\n",
    "                                                                                            #                           (1, batch_size, hidden_size) if batch_first=False\n",
    "\n",
    "        outputs = torch.zeros(L, batch_size, decoder_output_size, device=self.device) # have to reshape later if batch_first=True\n",
    "        decoder_input = torch.cat((x0, context_vec), dim=2)\n",
    "        x, h, c = self.decoder(decoder_input, h, c) # x.shape = (batch_size, 1, decoder.output_size) if batch_first=True\n",
    "                                                    #         = (1, batch_size, decoder.output_size) if batch_first=False\n",
    "        i = 1 if batch_first else 0\n",
    "        outputs[0] = x.squeeze(i)\n",
    "        for t in range(1, L):\n",
    "            context_vec = self.attention(h, encoder_hidden_states, batch_first)\n",
    "            context_vec = context_vec.unsqueeze(1) if batch_first else context_vec.unsqueeze(0) # context_vec.shape becomes (batch_size, 1, hidden_size) if batch_first=True\n",
    "                                                                                                #                           (1, batch_size, hidden_size) if batch_first=False\n",
    "            if random() > teacher_forcing_ratio:\n",
    "                x_ = transform(x)\n",
    "            else:\n",
    "                x_ = Y[t-1].unsqueeze(1) if batch_first else Y[t-1].unsqueeze(0)\n",
    "            decoder_input = torch.cat((x_, context_vec), dim=2)\n",
    "            x, h, c = self.decoder(decoder_input, h, c)\n",
    "            outputs[t] = x.squeeze(i)\n",
    "\n",
    "        if batch_first:\n",
    "            outputs = outputs.transpose(0,1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liA02WXV6SeF"
   },
   "outputs": [],
   "source": [
    "# batching from scratch, not used\n",
    "def batching(X, Y, batch_size, batch_first):\n",
    "    \"\"\"batches the data at each epoch; returns a list of batches for X and Y\"\"\"\n",
    "    # X.shape = (num_samples, L, encoder_input_size); Y.shape = (num_samples, L, decoder_output_size)\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = num_samples // batch_size + 1\n",
    "    indices = torch.randperm(num_samples)\n",
    "    indices_groups = [indices[i*batch_size: (i+1)*batch_size] for i in range(num_batches-1)]\n",
    "    indices_groups.append(indices[(num_batches-1)*batch_size:])\n",
    "    X_batched, Y_batched = [X[group] for group in indices_groups], [Y[group] for group in indices_groups]\n",
    "    if not batch_first:\n",
    "        X_batched, Y_batched = [x.permute(1,0,2) for x in X_batched], [y.permute(1,0,2) for y in Y_batched]\n",
    "    return X_batched, Y_batched\n",
    "\n",
    "\n",
    "def initializer(model, batch_size):\n",
    "    # h0 and c0 of shape (num_layers, batch_size, hidden_size); hidden_size = encoder_hidden_size = decoder_hidden_size\n",
    "    \"\"\"returns appropriate h0, c0 to initialize the encoder\"\"\"\n",
    "    h0 = torch.zeros(model.decoder.num_layers, batch_size, model.decoder.hidden_size).normal_(mean=0.0, std=0.01)\n",
    "    c0 = torch.zeros(model.decoder.num_layers, batch_size, model.decoder.hidden_size).normal_(mean=0.0, std=0.01)\n",
    "    return h0, c0\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, Y): # X.shape[0] = Y.shape[0] = num_samples\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "\n",
    "### needs revising for multivariate if input_size != output_size\n",
    "def train_seq2seq_model(model, criterion, optimizer, train_dataset: MyDataset,\n",
    "                        num_epochs: int, batch_size: int, transform: callable, teacher_forcing_ratio: float):\n",
    "    batch_first = model.encoder.batch_first\n",
    "    dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        print(f\"Epoch {epoch_idx+1}...\")\n",
    "        for _, (X_batch, Y_batch) in enumerate(dataloader): # X_batch.shape = (batch_size, L, encoder_input_size); Y_batch.shape = (batch_size, L, decoder_output_size)\n",
    "            # X_batch, Y_batch = X_batch.to(model.device), Y_batch.to(model.device)\n",
    "            ### this part for x0 initialization needs modifying\n",
    "            x0 = X_batch[:, -1, :]\n",
    "            if not batch_first:\n",
    "                X_batch, Y_batch = X_batch.transpose(0,1), Y_batch.transpose(0,1)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            h0, c0 = initializer(model, batch_size)\n",
    "            outputs = model(X_batch, x0, h0, c0,\n",
    "                            transform=transform,\n",
    "                            teacher_forcing_ratio=teacher_forcing_ratio,\n",
    "                            Y=Y_batch)\n",
    "            loss = criterion(outputs, Y_batch)\n",
    "            loss.backward() # back-propagation\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # avoids exploding gradients\n",
    "            optimizer.step()\n",
    "    print(\"Finished training.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
